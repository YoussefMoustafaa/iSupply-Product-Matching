{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from symspellpy import SymSpell\n",
    "from difflib import SequenceMatcher\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GRU, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku</th>\n",
       "      <th>marketplace_product_name_ar</th>\n",
       "      <th>seller_item_name</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1322</td>\n",
       "      <td>استوهالت 40 مجم 14 كبسول</td>\n",
       "      <td>ESTOHALT 40 MG 14 CAP</td>\n",
       "      <td>56.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1322</td>\n",
       "      <td>استوهالت 40 مجم 14 كبسول</td>\n",
       "      <td>استوهالت 40 مجم 14 ك</td>\n",
       "      <td>56.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1322</td>\n",
       "      <td>استوهالت 40 مجم 14 كبسول</td>\n",
       "      <td>استوهالت 40 مجم 14 ك</td>\n",
       "      <td>56.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1322</td>\n",
       "      <td>استوهالت 40 مجم 14 كبسول</td>\n",
       "      <td>استوهالت 40 مجم 14 ك</td>\n",
       "      <td>56.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1322</td>\n",
       "      <td>استوهالت 40 مجم 14 كبسول</td>\n",
       "      <td>استوهالت 40 مجم 14 ك</td>\n",
       "      <td>56.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1322</td>\n",
       "      <td>استوهالت 40 مجم 14 كبسول</td>\n",
       "      <td>استوهالت 40 مجم 14 ك</td>\n",
       "      <td>56.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1322</td>\n",
       "      <td>استوهالت 40 مجم 14 كبسول</td>\n",
       "      <td>استوهالت 40 مجم 14 ك</td>\n",
       "      <td>56.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1322</td>\n",
       "      <td>استوهالت 40 مجم 14 كبسول</td>\n",
       "      <td>استوهالت 40 مجم 1 شريط * 14 كبسولة</td>\n",
       "      <td>56.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1322</td>\n",
       "      <td>استوهالت 40 مجم 14 كبسول</td>\n",
       "      <td>استوهالت 40 مجم</td>\n",
       "      <td>56.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1322</td>\n",
       "      <td>استوهالت 40 مجم 14 كبسول</td>\n",
       "      <td>استوهالت 40 كبسول س ج</td>\n",
       "      <td>56.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sku marketplace_product_name_ar                    seller_item_name  price\n",
       "0  1322    استوهالت 40 مجم 14 كبسول               ESTOHALT 40 MG 14 CAP   56.5\n",
       "1  1322    استوهالت 40 مجم 14 كبسول                استوهالت 40 مجم 14 ك   56.5\n",
       "2  1322    استوهالت 40 مجم 14 كبسول                استوهالت 40 مجم 14 ك   56.5\n",
       "3  1322    استوهالت 40 مجم 14 كبسول                استوهالت 40 مجم 14 ك   56.5\n",
       "4  1322    استوهالت 40 مجم 14 كبسول                استوهالت 40 مجم 14 ك   56.5\n",
       "5  1322    استوهالت 40 مجم 14 كبسول                استوهالت 40 مجم 14 ك   56.5\n",
       "6  1322    استوهالت 40 مجم 14 كبسول                استوهالت 40 مجم 14 ك   56.5\n",
       "7  1322    استوهالت 40 مجم 14 كبسول  استوهالت 40 مجم 1 شريط * 14 كبسولة   56.5\n",
       "8  1322    استوهالت 40 مجم 14 كبسول                     استوهالت 40 مجم   56.5\n",
       "9  1322    استوهالت 40 مجم 14 كبسول               استوهالت 40 كبسول س ج   56.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"Product Matching Dataset.xlsx\" , sheet_name=\"Dataset\")\n",
    "# df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = set(stopwords.words(\"arabic\")) | set(stopwords.words(\"english\"))\n",
    "\n",
    "# # text_column = df[\"seller_item_name\"].astype(str)  # Ensure text format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[\\u064B-\\u065F]\", \"\", text)  # Remove Arabic diacritics (tashkeel)\n",
    "    # text = re.sub(r'[^\\u0621-\\u064A\\s]', '', text) # remove non-arabic characters\n",
    "    text = re.sub(r'[^\\u0621-\\u064A0-9\\s]', ' ', text) # remove non-arabic characters except for numbers\n",
    "    text = re.sub(r'\\s+', ' ', text) # remove multiple spaces\n",
    "\n",
    "    text = re.sub( r'قرص|\\bق\\b|\\bك\\b|اقراص|كبسوله', 'كبسول', text)\n",
    "    text = re.sub(r'([\\u0600-\\u06FF])\\1+', r'\\1', text) # remove arabic repetition\n",
    "    text = re.sub(r'[إأآ]', 'ا', text)\n",
    "    text = re.sub(r'ى', 'ي', text)\n",
    "    text = re.sub(r'ة', 'ه ', text)\n",
    "    text = re.sub(r'ؤ', 'و', text)\n",
    "    text = re.sub(r'ئ', 'ي', text)\n",
    "    text = re.sub(r'ــ', '', text)\n",
    "    text = re.sub(r\"(\\d+)\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"\\b[\\u0600-\\u06FF]\\b\", \"\", text)\n",
    "    text = re.sub(r'([\\u0600-\\u06FF])\\1{2,}', r'\\1\\1', text)\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text) # remove repetitions\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)  # Removes standalone single characters\n",
    "    text = re.sub(r'\\b(?:سعر جديد|س جديد|س جدي|س ج|ركز)\\b', '', text)# Remove specific unwanted phrases\n",
    "    text = re.sub( r'مرهم|اكريم', 'كريم', text)\n",
    "    text = re.sub(r'مليجرام|\\bم\\b|مجم', 'مجم', text)\n",
    "    text = re.sub(r'جرام|جم', 'جم', text)\n",
    "    text = re.sub( r'شرائط|شريطين', 'شريط', text)\n",
    "    text = re.sub( r'امبولات|امبوله|حقن', 'امبول', text)\n",
    "    text = re.sub( r'لبوس|لبوس اطفال', 'اقماع', text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip() # remove multiple spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qalsadi.lemmatizer import Lemmatizer\n",
    "\n",
    "# Initialize the Arabic lemmatizer\n",
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "def preprocessing(text):\n",
    "    # Convert to lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove Arabic diacritics (tashkeel)\n",
    "    text = re.sub(r\"[\\u064B-\\u065F]\", \"\", text)\n",
    "    \n",
    "    # Remove non-Arabic characters except numbers\n",
    "    text = re.sub(r'[^\\u0621-\\u064A0-9\\s]', ' ', text)\n",
    "    \n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Standardize specific terms\n",
    "    text = re.sub(r'قرص|\\bق\\b|\\bك\\b|اقراص|كبسوله', 'كبسول', text)\n",
    "    text = re.sub(r'([\\u0600-\\u06FF])\\1+', r'\\1', text)  # Remove Arabic repetition\n",
    "    text = re.sub(r'[إأآ]', 'ا', text)\n",
    "    text = re.sub(r'ى', 'ي', text)\n",
    "    text = re.sub(r'ة', 'ه ', text)\n",
    "    text = re.sub(r'ؤ', 'و', text)\n",
    "    text = re.sub(r'ئ', 'ي', text)\n",
    "    \n",
    "    # Separate numbers that stick to Arabic/English words\n",
    "    text = re.sub(r\"(\\d+)([a-zA-Z\\u0600-\\u06FF]+)\", r\"\\1 \\2\", text)  # Number followed by Arabic/English\n",
    "    text = re.sub(r\"([a-zA-Z\\u0600-\\u06FF]+)(\\d+)\", r\"\\1 \\2\", text)  # Arabic/English followed by number\n",
    "    \n",
    "    # Remove standalone Arabic/English characters (but not numbers)\n",
    "    text = re.sub(r\"\\b[^\\W\\d]\\b\", \"\", text)\n",
    "    \n",
    "    # Remove specific unwanted phrases\n",
    "    text = re.sub(r'\\b(?:سعر جديد|سعر|قديم|س جديد|س جدي|س ج|ركز)\\b', '', text)\n",
    "    text = re.sub(r'مرهم|اكريم', 'كريم', text)\n",
    "    text = re.sub(r'مليجرام|\\bم\\b|مجم', 'مجم', text)\n",
    "    text = re.sub(r'جرام|جم', 'جم', text)\n",
    "    text = re.sub(r'شرائط|شريطين', 'شريط', text)\n",
    "    text = re.sub(r'امبولات|امبوله|حقن', 'امبول', text)\n",
    "    text = re.sub(r'لبوس|لبوس اطفال', 'اقماع', text)\n",
    "    \n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Lemmatize each token\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join the lemmatized tokens back into a single string\n",
    "    lemmatized_text = \" \".join(lemmatized_tokens)\n",
    "    \n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hassa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m2090/2090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 29ms/step - accuracy: 0.1191 - loss: 4.7483 - val_accuracy: 0.8717 - val_loss: 1.0730\n",
      "Epoch 2/100\n",
      "\u001b[1m2090/2090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 21ms/step - accuracy: 0.8630 - loss: 0.9335 - val_accuracy: 0.9743 - val_loss: 0.2818\n",
      "Epoch 3/100\n",
      "\u001b[1m2090/2090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 22ms/step - accuracy: 0.9615 - loss: 0.3295 - val_accuracy: 0.9793 - val_loss: 0.1942\n",
      "Epoch 4/100\n",
      "\u001b[1m2090/2090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 22ms/step - accuracy: 0.9747 - loss: 0.2020 - val_accuracy: 0.9793 - val_loss: 0.1604\n",
      "Epoch 5/100\n",
      "\u001b[1m2090/2090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 22ms/step - accuracy: 0.9785 - loss: 0.1450 - val_accuracy: 0.9800 - val_loss: 0.1511\n",
      "Epoch 6/100\n",
      "\u001b[1m2090/2090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 22ms/step - accuracy: 0.9793 - loss: 0.1231 - val_accuracy: 0.9807 - val_loss: 0.1459\n",
      "Epoch 7/100\n",
      "\u001b[1m2090/2090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 24ms/step - accuracy: 0.9817 - loss: 0.1055 - val_accuracy: 0.9804 - val_loss: 0.1487\n",
      "Epoch 8/100\n",
      "\u001b[1m2090/2090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 22ms/step - accuracy: 0.9839 - loss: 0.0888 - val_accuracy: 0.9812 - val_loss: 0.1468\n",
      "Epoch 9/100\n",
      "\u001b[1m2090/2090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 22ms/step - accuracy: 0.9836 - loss: 0.0877 - val_accuracy: 0.9817 - val_loss: 0.1516\n",
      "Epoch 10/100\n",
      "\u001b[1m2090/2090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 24ms/step - accuracy: 0.9835 - loss: 0.0860 - val_accuracy: 0.9816 - val_loss: 0.1497\n",
      "Epoch 11/100\n",
      "\u001b[1m2090/2090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 22ms/step - accuracy: 0.9856 - loss: 0.0764 - val_accuracy: 0.9819 - val_loss: 0.1478\n",
      "\u001b[1m523/523\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9796 - loss: 0.1559\n",
      "RNN Model Accuracy: 0.9807\n"
     ]
    }
   ],
   "source": [
    "df[\"cleaned_seller_item_name\"] = df[\"seller_item_name\"].astype(str).apply(preprocessing)\n",
    "\n",
    "# Scale Price\n",
    "scaler = StandardScaler()\n",
    "df[\"scaled_price\"] = scaler.fit_transform(df[[\"price\"]])\n",
    "\n",
    "# Feature and Target\n",
    "X = df[[\"cleaned_seller_item_name\", \"scaled_price\"]]\n",
    "Y = df[\"sku\"]\n",
    "\n",
    "# Label Encoding for SKU\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"encoded_sku\"] = label_encoder.fit_transform(Y)\n",
    "\n",
    "# Tokenization for RNN\n",
    "num_words = 5000\n",
    "tokenizer = Tokenizer(num_words=num_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X[\"cleaned_seller_item_name\"])\n",
    "sequences = tokenizer.texts_to_sequences(X[\"cleaned_seller_item_name\"])\n",
    "\n",
    "# Padding\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "x_padded = pad_sequences(sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "# Train-Test Split for RNN\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_padded, df[\"encoded_sku\"], test_size=0.2, random_state=42, stratify=df[\"encoded_sku\"], shuffle=True)\n",
    "\n",
    "# Build Improved RNN Model\n",
    "rnn_model = Sequential([\n",
    "    Embedding(input_dim=num_words, output_dim=256, input_length=max_length),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Dropout(0.2),\n",
    "    Bidirectional(GRU(32)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation=\"relu\"),\n",
    "    Dense(len(label_encoder.classes_), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "rnn_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=Adam(learning_rate=0.0005), metrics=[\"accuracy\"])\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train RNN Model\n",
    "rnn_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate RNN Model\n",
    "loss, accuracy = rnn_model.evaluate(X_test, y_test)\n",
    "print(f\"RNN Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cleaned_seller_item_name\"] = df[\"seller_item_name\"].astype(str).apply(preprocessing)\n",
    "scaler = StandardScaler()\n",
    "df[\"scaled_price\"] = scaler.fit_transform(df[[\"price\"]])\n",
    "X = df[[\"cleaned_seller_item_name\", \"scaled_price\"]]\n",
    "Y = df[\"sku\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'شوجارلو بلس 1000 50 مج 30 كبسول الاسراء'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text  = preprocessing(\"تورسيرتيك5مجم\")\n",
    "# text  = preprocessing(\"تورسيرتيك 5مجم\")\n",
    "# text  = preprocessing(\"SPANIELA MR35MG30TAB\")\n",
    "# text  = preprocessing(\"شوجارلو بلس 1000/50مج30قرص س ج/الاسراء\")\n",
    "# text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build SymSpell Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Column 'marketplace_product_name_ar' not found in dataset!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 85\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(dataset_filename):\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found! Check the filename and path.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 85\u001b[0m dict_file \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_dictionary_from_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m sym_spell \u001b[38;5;241m=\u001b[39m initialize_symspell(dict_file)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Example: Input Medicine Name and Correct It\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[45], line 50\u001b[0m, in \u001b[0;36mbuild_dictionary_from_dataset\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Ensure column exists\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmarketplace_product_name_ar\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m---> 50\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmarketplace_product_name_ar\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found in dataset!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m product \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmarketplace_product_name_ar\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdropna():\n\u001b[0;32m     53\u001b[0m     normalized_product \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(product)\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mValueError\u001b[0m: Column 'marketplace_product_name_ar' not found in dataset!"
     ]
    }
   ],
   "source": [
    "# from symspellpy import SymSpell\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load DataFrame\n",
    "# df = pd.DataFrame({\n",
    "#     \"marketplace_product_name_ar\": [\"منتج\", \"منتج\", \"منتج جديد\", \"منج جديد\"],  # Example Data\n",
    "#     \"cleaned_seller_item_name\": [\"منتج جد\", \"منتج جدي\", \"منتج جديي\", \"منج جد\"]\n",
    "# })\n",
    "\n",
    "# # 1. Create Word Frequency Dictionary\n",
    "# word_freq_dict = {}\n",
    "# for product in df[\"marketplace_product_name_ar\"].dropna():\n",
    "#     normalized_product = str(product).strip()\n",
    "#     word_freq_dict[normalized_product] = word_freq_dict.get(normalized_product, 0) + 1\n",
    "\n",
    "# # Save Dictionary (Ensure tab separation)\n",
    "# dict_file = \"product_dictionary.txt\"\n",
    "# with open(dict_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#     for word, freq in word_freq_dict.items():\n",
    "#         f.write(f\"{word}\\t{freq}\\n\")  # Use TAB instead of SPACE\n",
    "\n",
    "# # 2. Initialize SymSpell\n",
    "# sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "\n",
    "# # Load Dictionary and Check if Loaded Correctly\n",
    "# if not sym_spell.load_dictionary(dict_file, term_index=0, count_index=1, encoding=\"utf-8\"):\n",
    "#     print(\"Dictionary loading failed!\")\n",
    "\n",
    "# # 3. Apply SymSpell Correction\n",
    "# def correct_text(text):\n",
    "#     suggestions = sym_spell.lookup(text, verbosity=0, max_edit_distance=2)  # Use verbosity=0 for best match\n",
    "#     return suggestions[0].term if suggestions else text\n",
    "\n",
    "# df[\"corrected_seller_item_name\"] = df[\"cleaned_seller_item_name\"].apply(correct_text)\n",
    "\n",
    "# # Show Results\n",
    "# print(df[[\"cleaned_seller_item_name\", \"corrected_seller_item_name\"]])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sku', 'marketplace_product_name_ar', 'seller_item_name', 'price']\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# dataset_filename = \"Product Matching Dataset.xlsx\"  # Your file name\n",
    "\n",
    "# df = pd.read_excel(dataset_filename, sheet_name=\"Dataset\")  # Load Excel file\n",
    "# print(df.columns.tolist())  # Print all column names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.8627\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=500)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train[\"cleaned_seller_item_name\"])\n",
    "X_test_tfidf = vectorizer.transform(X_test[\"cleaned_seller_item_name\"])\n",
    "\n",
    "# # Train Logistic Regression Model\n",
    "# logistic_model = LogisticRegression()\n",
    "# logistic_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# y_pred = logistic_model.predict(X_test_tfidf)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Logistic Regression Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Accuracy: 0.8648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hassa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1828/1828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 21ms/step - accuracy: 0.1221 - loss: 4.7405 - val_accuracy: 0.8558 - val_loss: 1.0872\n",
      "Epoch 2/100\n",
      "\u001b[1m1828/1828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 22ms/step - accuracy: 0.8361 - loss: 0.9156 - val_accuracy: 0.9420 - val_loss: 0.2969\n",
      "Epoch 3/100\n",
      "\u001b[1m1828/1828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 22ms/step - accuracy: 0.9329 - loss: 0.3006 - val_accuracy: 0.9500 - val_loss: 0.2200\n",
      "Epoch 4/100\n",
      "\u001b[1m1828/1828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 21ms/step - accuracy: 0.9475 - loss: 0.1839 - val_accuracy: 0.9501 - val_loss: 0.2161\n",
      "Epoch 5/100\n",
      "\u001b[1m1828/1828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 21ms/step - accuracy: 0.9503 - loss: 0.1493 - val_accuracy: 0.9528 - val_loss: 0.2165\n",
      "Epoch 6/100\n",
      "\u001b[1m1828/1828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 22ms/step - accuracy: 0.9564 - loss: 0.1233 - val_accuracy: 0.9559 - val_loss: 0.2106\n",
      "Epoch 7/100\n",
      "\u001b[1m1828/1828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 22ms/step - accuracy: 0.9583 - loss: 0.1090 - val_accuracy: 0.9558 - val_loss: 0.2159\n",
      "Epoch 8/100\n",
      "\u001b[1m1828/1828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 21ms/step - accuracy: 0.9610 - loss: 0.1031 - val_accuracy: 0.9545 - val_loss: 0.2266\n",
      "Epoch 9/100\n",
      "\u001b[1m1828/1828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 21ms/step - accuracy: 0.9608 - loss: 0.0951 - val_accuracy: 0.9566 - val_loss: 0.2254\n",
      "Epoch 10/100\n",
      "\u001b[1m1828/1828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 22ms/step - accuracy: 0.9618 - loss: 0.0940 - val_accuracy: 0.9562 - val_loss: 0.2317\n",
      "Epoch 11/100\n",
      "\u001b[1m1828/1828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 28ms/step - accuracy: 0.9615 - loss: 0.0912 - val_accuracy: 0.9562 - val_loss: 0.2347\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9566 - loss: 0.2083\n",
      "RNN Model Accuracy: 0.9559\n"
     ]
    }
   ],
   "source": [
    "# label_encoder = LabelEncoder()\n",
    "# df[\"encoded_sku\"] = label_encoder.fit_transform(df[\"sku\"])\n",
    "\n",
    "# # Tokenization for RNN\n",
    "# num_words = 5000\n",
    "# tokenizer = Tokenizer(num_words=num_words, oov_token=\"<OOV>\")\n",
    "# tokenizer.fit_on_texts(df[\"cleaned_seller_item_name\"])\n",
    "# sequences = tokenizer.texts_to_sequences(df[\"cleaned_seller_item_name\"])\n",
    "\n",
    "# # Padding\n",
    "# max_length = max(len(seq) for seq in sequences)\n",
    "# x_padded = pad_sequences(sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "# # Train-Test Split for RNN\n",
    "# X_train, X_test, y_train, y_test = train_test_split(x_padded, df[\"encoded_sku\"], test_size=0.3, random_state=42)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_padded, y_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# rnn_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "# # Early Stopping\n",
    "# early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "# # Train RNN Model\n",
    "# rnn_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# loss, accuracy = rnn_model.evaluate(X_test, y_test)\n",
    "# print(f\"RNN Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# Label Encoding for SKU\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"encoded_sku\"] = label_encoder.fit_transform(df[\"sku\"])\n",
    "\n",
    "# # Tokenization for RNN\n",
    "# num_words = 5000\n",
    "# tokenizer = Tokenizer(num_words=num_words, oov_token=\"<OOV>\")\n",
    "# tokenizer.fit_on_texts(X)\n",
    "# sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# # Padding\n",
    "# max_length = max(len(seq) for seq in sequences)\n",
    "# x_padded = pad_sequences(sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "# Train-Test Split for RNN\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_padded, df[\"encoded_sku\"], test_size=0.3, random_state=42)\n",
    "\n",
    "# Build Improved RNN Model\n",
    "rnn_model = Sequential([\n",
    "    Embedding(input_dim=num_words, output_dim=256, input_length=max_length),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Dropout(0.2),\n",
    "    Bidirectional(GRU(32)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation=\"relu\"),\n",
    "    Dense(len(label_encoder.classes_), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# rnn_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=Adam(learning_rate=0.0005), metrics=[\"accuracy\"])\n",
    "\n",
    "# # Early Stopping\n",
    "# early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "# # Train RNN Model\n",
    "# rnn_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# loss, accuracy = rnn_model.evaluate(X_test, y_test)\n",
    "# print(f\"RNN Model Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "1469\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "116\n"
     ]
    }
   ],
   "source": [
    "def predict_sku(medicine_name, threshold=0.5):\n",
    "    # Preprocess the input name\n",
    "    cleaned_text = preprocessing(medicine_name)\n",
    "    \n",
    "    # Convert to sequence\n",
    "    seq = tokenizer.texts_to_sequences([cleaned_text])\n",
    "    padded_seq = pad_sequences(seq, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "    # Get model predictions\n",
    "    predictions = rnn_model.predict(padded_seq)[0]\n",
    "    \n",
    "    # Get highest probability SKU\n",
    "    max_prob = np.max(predictions)\n",
    "    predicted_label = np.argmax(predictions)\n",
    "\n",
    "    # If confidence is low, return \"unknown\"\n",
    "    if max_prob < threshold:\n",
    "        return \"unknown\"\n",
    "\n",
    "    # Convert label back to SKU\n",
    "    return label_encoder.inverse_transform([predicted_label])[0]\n",
    "\n",
    "# # Example Predictions\n",
    "# print(predict_sku(\"بانادول اكسترا\"))  # Should return the correct SKU or \"unknown\"\n",
    "# print(predict_sku(\"شوجارلو بلس 50/1000  30\"))  # Should return \"unknown\"\n",
    "# print(predict_sku(\"GLIPTUS PLUS 50/1000 MG 30 TAB\"))  # Should return \"unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step\n"
     ]
    }
   ],
   "source": [
    "probabilities = rnn_model.predict(X_test)\n",
    "\n",
    "confidence_scores = np.max(probabilities, axis=1)\n",
    "\n",
    "predicted_indices = np.argmax(probabilities, axis=1)\n",
    "\n",
    "predicted_labels = label_encoder.inverse_transform(predicted_indices)\n",
    "\n",
    "confidence_threshold = 0.85\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    confidence = confidence_scores[i]\n",
    "    predicted_class = predicted_labels[i]\n",
    "\n",
    "    if confidence < confidence_threshold:\n",
    "        predicted_class = \"Unknown\"\n",
    "\n",
    "    test_results.append({\n",
    "        'Predicted': predicted_class,\n",
    "        'Confidence': f\"{confidence:.2f}\"\n",
    "    })\n",
    "\n",
    "temp_df = pd.DataFrame(test_results)\n",
    "\n",
    "temp_df.head()\n",
    "df.to_csv(\"predict.csv\" , index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting farasa\n",
      "  Downloading Farasa-0.0.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Downloading Farasa-0.0.1-py2.py3-none-any.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/12.6 MB 645.7 kB/s eta 0:00:19\n",
      "   - -------------------------------------- 0.5/12.6 MB 645.7 kB/s eta 0:00:19\n",
      "   -- ------------------------------------- 0.8/12.6 MB 657.8 kB/s eta 0:00:18\n",
      "   -- ------------------------------------- 0.8/12.6 MB 657.8 kB/s eta 0:00:18\n",
      "   --- ------------------------------------ 1.0/12.6 MB 645.7 kB/s eta 0:00:18\n",
      "   --- ------------------------------------ 1.0/12.6 MB 645.7 kB/s eta 0:00:18\n",
      "   --- ------------------------------------ 1.0/12.6 MB 645.7 kB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 1.3/12.6 MB 583.5 kB/s eta 0:00:20\n",
      "   ---- ----------------------------------- 1.3/12.6 MB 583.5 kB/s eta 0:00:20\n",
      "   ---- ----------------------------------- 1.6/12.6 MB 590.9 kB/s eta 0:00:19\n",
      "   ----- ---------------------------------- 1.8/12.6 MB 625.4 kB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 1.8/12.6 MB 625.4 kB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 1.8/12.6 MB 625.4 kB/s eta 0:00:18\n",
      "   ------ --------------------------------- 2.1/12.6 MB 611.7 kB/s eta 0:00:18\n",
      "   ------ --------------------------------- 2.1/12.6 MB 611.7 kB/s eta 0:00:18\n",
      "   ------- -------------------------------- 2.4/12.6 MB 607.3 kB/s eta 0:00:17\n",
      "   -------- ------------------------------- 2.6/12.6 MB 626.6 kB/s eta 0:00:16\n",
      "   -------- ------------------------------- 2.6/12.6 MB 626.6 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.9/12.6 MB 628.3 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.9/12.6 MB 628.3 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.9/12.6 MB 628.3 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 3.1/12.6 MB 615.2 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 3.1/12.6 MB 615.2 kB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 3.4/12.6 MB 621.4 kB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 3.4/12.6 MB 621.4 kB/s eta 0:00:15\n",
      "   ----------- ---------------------------- 3.7/12.6 MB 614.4 kB/s eta 0:00:15\n",
      "   ----------- ---------------------------- 3.7/12.6 MB 614.4 kB/s eta 0:00:15\n",
      "   ------------ --------------------------- 3.9/12.6 MB 618.2 kB/s eta 0:00:14\n",
      "   ------------ --------------------------- 3.9/12.6 MB 618.2 kB/s eta 0:00:14\n",
      "   ------------- -------------------------- 4.2/12.6 MB 613.8 kB/s eta 0:00:14\n",
      "   ------------- -------------------------- 4.2/12.6 MB 613.8 kB/s eta 0:00:14\n",
      "   -------------- ------------------------- 4.5/12.6 MB 611.5 kB/s eta 0:00:14\n",
      "   -------------- ------------------------- 4.5/12.6 MB 611.5 kB/s eta 0:00:14\n",
      "   -------------- ------------------------- 4.7/12.6 MB 618.7 kB/s eta 0:00:13\n",
      "   --------------- ------------------------ 5.0/12.6 MB 622.7 kB/s eta 0:00:13\n",
      "   --------------- ------------------------ 5.0/12.6 MB 622.7 kB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 5.2/12.6 MB 623.8 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 5.2/12.6 MB 623.8 kB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 5.5/12.6 MB 622.5 kB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 5.5/12.6 MB 622.5 kB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 5.5/12.6 MB 622.5 kB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 5.5/12.6 MB 622.5 kB/s eta 0:00:12\n",
      "   ------------------ --------------------- 5.8/12.6 MB 599.2 kB/s eta 0:00:12\n",
      "   ------------------ --------------------- 5.8/12.6 MB 599.2 kB/s eta 0:00:12\n",
      "   ------------------- -------------------- 6.0/12.6 MB 598.3 kB/s eta 0:00:11\n",
      "   ------------------- -------------------- 6.0/12.6 MB 598.3 kB/s eta 0:00:11\n",
      "   ------------------- -------------------- 6.3/12.6 MB 602.0 kB/s eta 0:00:11\n",
      "   ------------------- -------------------- 6.3/12.6 MB 602.0 kB/s eta 0:00:11\n",
      "   -------------------- ------------------- 6.6/12.6 MB 603.7 kB/s eta 0:00:10\n",
      "   -------------------- ------------------- 6.6/12.6 MB 603.7 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 6.8/12.6 MB 601.8 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 6.8/12.6 MB 601.8 kB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 7.1/12.6 MB 601.7 kB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 7.1/12.6 MB 601.7 kB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 7.3/12.6 MB 601.6 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 7.3/12.6 MB 601.6 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 7.6/12.6 MB 604.6 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 7.6/12.6 MB 604.6 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 7.9/12.6 MB 604.4 kB/s eta 0:00:08\n",
      "   ------------------------ --------------- 7.9/12.6 MB 604.4 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 8.1/12.6 MB 608.6 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 8.1/12.6 MB 608.6 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 8.4/12.6 MB 611.1 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 8.4/12.6 MB 611.1 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 8.7/12.6 MB 606.6 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 8.7/12.6 MB 606.6 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 8.9/12.6 MB 611.1 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 8.9/12.6 MB 611.1 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 9.2/12.6 MB 609.4 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 9.2/12.6 MB 609.4 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 9.4/12.6 MB 614.2 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 9.4/12.6 MB 614.2 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 9.7/12.6 MB 611.9 kB/s eta 0:00:05\n",
      "   ------------------------------ --------- 9.7/12.6 MB 611.9 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 10.0/12.6 MB 611.6 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 10.0/12.6 MB 611.6 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 10.2/12.6 MB 615.4 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 10.2/12.6 MB 615.4 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 10.5/12.6 MB 610.9 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 10.5/12.6 MB 610.9 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 10.7/12.6 MB 613.5 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 10.7/12.6 MB 613.5 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 11.0/12.6 MB 616.4 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 11.0/12.6 MB 616.4 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 11.3/12.6 MB 615.9 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 11.3/12.6 MB 615.9 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 11.5/12.6 MB 615.6 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 11.5/12.6 MB 615.6 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 11.8/12.6 MB 614.7 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 11.8/12.6 MB 614.7 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 12.1/12.6 MB 615.3 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.1/12.6 MB 615.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------  12.3/12.6 MB 614.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------  12.3/12.6 MB 614.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 617.1 kB/s eta 0:00:00\n",
      "Installing collected packages: farasa\n",
      "Successfully installed farasa-0.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# pip install farasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'farasa.segmenter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfarasa\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msegmenter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FarasaSegmenter\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcorrect_spelling_arabic\u001b[39m(text):\n\u001b[0;32m      4\u001b[0m     segmenter \u001b[38;5;241m=\u001b[39m FarasaSegmenter()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'farasa.segmenter'"
     ]
    }
   ],
   "source": [
    "# from farasa.segmenter import FarasaSegmenter\n",
    "\n",
    "# def correct_spelling_arabic(text):\n",
    "#     segmenter = FarasaSegmenter()\n",
    "#     corrected_text = segmenter.spellcheck(text)\n",
    "#     return corrected_text\n",
    "\n",
    "# # Test\n",
    "# text = \"فلكسوفان كيسول س ج ركزز\"\n",
    "# corrected_text = correct_spelling_arabic(text)\n",
    "# print(corrected_text)  # Output: \"فليكسوفان كبسول س ج ركزز\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
